{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Writing_Like_Shakespeare.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNPnRQ3+UsjL9cpCSE+kws0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S-MD-Fouzan/Natural-Language-Processing/blob/master/Writing_Like_Shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMg9LaBrgQ2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.utils as ku \n",
        "import numpy as np "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI0BVWd1grxy",
        "colab_type": "code",
        "outputId": "3b7c0bad-8b1e-446f-9387-8fd4b2d306eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\\n",
        "    -O /tmp/sonnets.txt\n",
        "data = open('/tmp/sonnets.txt').read()\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# create input sequences using list of tokens\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "label = ku.to_categorical(label, num_classes=total_words)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-04 15:57:04--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.210.128, 2607:f8b0:400c:c07::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.210.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 93578 (91K) [text/plain]\n",
            "Saving to: ‘/tmp/sonnets.txt’\n",
            "\n",
            "\r/tmp/sonnets.txt      0%[                    ]       0  --.-KB/s               \r/tmp/sonnets.txt    100%[===================>]  91.38K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2020-06-04 15:57:04 (138 MB/s) - ‘/tmp/sonnets.txt’ saved [93578/93578]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b37Bd3vFC0uR",
        "colab_type": "code",
        "outputId": "9d51fb62-318f-4cb8-f8dc-097d7e7cf1f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 10, 100)           321100    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 10, 300)           301200    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 10, 300)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1605)              162105    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3211)              5156866   \n",
            "=================================================================\n",
            "Total params: 6,101,671\n",
            "Trainable params: 6,101,671\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCqP6bJeDQX5",
        "colab_type": "code",
        "outputId": "f60d4157-f0b0-41c1-c6a8-987de2ec48b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " history = model.fit(predictors, label, epochs=100, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "484/484 [==============================] - 49s 102ms/step - loss: 6.9065 - accuracy: 0.0195\n",
            "Epoch 2/100\n",
            "484/484 [==============================] - 50s 103ms/step - loss: 6.5039 - accuracy: 0.0228\n",
            "Epoch 3/100\n",
            "484/484 [==============================] - 50s 102ms/step - loss: 6.4034 - accuracy: 0.0252\n",
            "Epoch 4/100\n",
            "484/484 [==============================] - 49s 102ms/step - loss: 6.2851 - accuracy: 0.0288\n",
            "Epoch 5/100\n",
            "484/484 [==============================] - 49s 102ms/step - loss: 6.1977 - accuracy: 0.0352\n",
            "Epoch 6/100\n",
            "484/484 [==============================] - 49s 102ms/step - loss: 6.1201 - accuracy: 0.0369\n",
            "Epoch 7/100\n",
            "484/484 [==============================] - 50s 102ms/step - loss: 6.0491 - accuracy: 0.0419\n",
            "Epoch 8/100\n",
            "484/484 [==============================] - 49s 102ms/step - loss: 5.9740 - accuracy: 0.0446\n",
            "Epoch 9/100\n",
            "484/484 [==============================] - 49s 102ms/step - loss: 5.8842 - accuracy: 0.0481\n",
            "Epoch 10/100\n",
            "484/484 [==============================] - 49s 102ms/step - loss: 5.7777 - accuracy: 0.0550\n",
            "Epoch 11/100\n",
            "484/484 [==============================] - 49s 102ms/step - loss: 5.6627 - accuracy: 0.0629\n",
            "Epoch 12/100\n",
            "484/484 [==============================] - 50s 102ms/step - loss: 5.5456 - accuracy: 0.0680\n",
            "Epoch 13/100\n",
            "484/484 [==============================] - 50s 102ms/step - loss: 5.4316 - accuracy: 0.0751\n",
            "Epoch 14/100\n",
            "484/484 [==============================] - 50s 102ms/step - loss: 5.3193 - accuracy: 0.0797\n",
            "Epoch 15/100\n",
            "484/484 [==============================] - 49s 102ms/step - loss: 5.2112 - accuracy: 0.0849\n",
            "Epoch 16/100\n",
            "484/484 [==============================] - 49s 102ms/step - loss: 5.1018 - accuracy: 0.0953\n",
            "Epoch 17/100\n",
            "484/484 [==============================] - 50s 103ms/step - loss: 4.9985 - accuracy: 0.1017\n",
            "Epoch 18/100\n",
            "484/484 [==============================] - 49s 102ms/step - loss: 4.8963 - accuracy: 0.1068\n",
            "Epoch 19/100\n",
            "484/484 [==============================] - 49s 102ms/step - loss: 4.7922 - accuracy: 0.1159\n",
            "Epoch 20/100\n",
            "484/484 [==============================] - 49s 102ms/step - loss: 4.6864 - accuracy: 0.1258\n",
            "Epoch 21/100\n",
            "484/484 [==============================] - 49s 102ms/step - loss: 4.5759 - accuracy: 0.1358\n",
            "Epoch 22/100\n",
            "484/484 [==============================] - 49s 101ms/step - loss: 4.4774 - accuracy: 0.1488\n",
            "Epoch 23/100\n",
            "484/484 [==============================] - 49s 102ms/step - loss: 4.3741 - accuracy: 0.1590\n",
            "Epoch 24/100\n",
            "484/484 [==============================] - 49s 102ms/step - loss: 4.2711 - accuracy: 0.1677\n",
            "Epoch 25/100\n",
            "484/484 [==============================] - 49s 101ms/step - loss: 4.1690 - accuracy: 0.1838\n",
            "Epoch 26/100\n",
            "484/484 [==============================] - 49s 102ms/step - loss: 4.0727 - accuracy: 0.1953\n",
            "Epoch 27/100\n",
            "484/484 [==============================] - 50s 103ms/step - loss: 3.9777 - accuracy: 0.2111\n",
            "Epoch 28/100\n",
            "484/484 [==============================] - 52s 108ms/step - loss: 3.8798 - accuracy: 0.2281\n",
            "Epoch 29/100\n",
            "484/484 [==============================] - 52s 108ms/step - loss: 3.7835 - accuracy: 0.2434\n",
            "Epoch 30/100\n",
            "484/484 [==============================] - 51s 105ms/step - loss: 3.6905 - accuracy: 0.2622\n",
            "Epoch 31/100\n",
            "484/484 [==============================] - 51s 105ms/step - loss: 3.5997 - accuracy: 0.2805\n",
            "Epoch 32/100\n",
            "484/484 [==============================] - 51s 105ms/step - loss: 3.5177 - accuracy: 0.2958\n",
            "Epoch 33/100\n",
            "484/484 [==============================] - 51s 105ms/step - loss: 3.4285 - accuracy: 0.3181\n",
            "Epoch 34/100\n",
            "484/484 [==============================] - 51s 105ms/step - loss: 3.3343 - accuracy: 0.3362\n",
            "Epoch 35/100\n",
            "484/484 [==============================] - 50s 104ms/step - loss: 3.2583 - accuracy: 0.3529\n",
            "Epoch 36/100\n",
            "484/484 [==============================] - 50s 104ms/step - loss: 3.1784 - accuracy: 0.3680\n",
            "Epoch 37/100\n",
            "484/484 [==============================] - 50s 104ms/step - loss: 3.1113 - accuracy: 0.3826\n",
            "Epoch 38/100\n",
            "484/484 [==============================] - 50s 103ms/step - loss: 3.0290 - accuracy: 0.4053\n",
            "Epoch 39/100\n",
            "484/484 [==============================] - 50s 103ms/step - loss: 2.9657 - accuracy: 0.4166\n",
            "Epoch 40/100\n",
            " 15/484 [..............................] - ETA: 45s - loss: 2.7929 - accuracy: 0.4938"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOh7jIljDdOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.title('Training accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhnvhICKDghX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\tprint(token_list)\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}